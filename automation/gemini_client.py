# -*- coding: utf-8 -*-
import os
import base64
import json
from google import genai
from google.genai import types
from dotenv import load_dotenv
import time
import random
import textwrap


load_dotenv(override=True)

class GeminiClient:
    def __init__(self):
        self.project_id = os.getenv("GOOGLE_CLOUD_PROJECT")
        self.location = os.getenv("GOOGLE_CLOUD_LOCATION")
        self.api_key = os.getenv("GEMINI_API_KEY")
        self.client = None
        self.use_vertex = False

        # Prioritize Vertex AI initialization
        if self.project_id and self.location:
            try:
                print(f"Initializing Gemini with Vertex AI (Project: {self.project_id}, Location: {self.location})")
                self.client = genai.Client(vertexai=True, project=self.project_id, location=self.location)
                self.use_vertex = True
            except Exception as e:
                print(f"Warning: Vertex AI initialization failed: {e}, falling back to API Key.")
                self.use_vertex = False
        
        # Fallback to API Key if Vertex AI is not available
        if not self.use_vertex:
            if self.api_key:
                print("Initializing Gemini with API Key (google-genai)")
                self.client = genai.Client(api_key=self.api_key)
            else:
                raise ValueError("Missing Gemini credentials. Set GOOGLE_CLOUD_PROJECT/LOCATION or GEMINI_API_KEY in .env")

    def _retry_request(self, func, *args, **kwargs):
        """
        Retry a function call with exponential backoff if a quota error occurs.
        """
        max_retries = 5
        base_delay = 2  # seconds
        
        for attempt in range(max_retries):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                error_str = str(e).lower()
                # Check for rate limit/quota errors
                if "429" in error_str or "quota" in error_str or "exhausted" in error_str:
                    if attempt == max_retries - 1:
                        print(f"Max retries ({max_retries}) exceeded for quota error.")
                        raise e
                    
                    delay = (base_delay * (2 ** attempt)) + (random.random() * 1)
                    print(f"Quota exceeded (429). Retrying in {delay:.2f}s... (Attempt {attempt + 1}/{max_retries})")
                    time.sleep(delay)
                else:
                    # Not a quota error, raise immediately
                    raise e

    def generate_content(self, prompt, model='gemini-3-pro-preview', config=None):
        """
        Generic method to generate content with retry logic.
        """
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model=model,
                contents=prompt,
                config=config
            )
            return response
        except Exception as e:
            print(f"Error generating content: {e}")
            return None

    def generate_article(self, keyword, article_type="topic-focus", context=None, extra_instructions=None, category=None):
        """
        Generate a full blog article in Markdown format.
        
        Args:
            keyword: Main topic or title.
            article_type: "topic-focus" (Deep Dive) is the primary type.
        """
        print(f"Generating article for keyword: {keyword} (Type: {article_type}, Category: {category})")
        
        # Normalize type: 'featured-news' -> 'topic-focus'
        if article_type == 'featured-news':
            article_type = 'topic-focus'
            
        context_section = ""
        if context:
            context_section = f"""
            ## Context Information
            The following external information is relevant to the topic. Use it to ensure accuracy and freshness.
            Summary: {context.get('summary', '')}
            Key Facts: {', '.join(context.get('key_facts', []))}
            Analyst View: {context.get('finshift_view', '')}
            """
            
        prompts = {
            # --- TechShift Primary Prompt ---
            # [Topic Deep Dive] Single article focus.
            "topic-focus": textwrap.dedent(f"""
            {context_section}ã‚ãªãŸã¯å°‚é–€æŠ€è¡“ã‚¢ãƒŠãƒªã‚¹ãƒˆï¼ˆTech Analystï¼‰ã§ã™ã€‚
            ç‰¹å®šã®æŠ€è¡“ãƒˆãƒ”ãƒƒã‚¯ï¼ˆ{keyword}ï¼‰ã«ã¤ã„ã¦ã€ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã¸ã®å½±éŸ¿ã‚’æ·±æ˜ã‚Šã™ã‚‹è§£èª¬è¨˜äº‹ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ã€‚
            
            ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}
            
            ## ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            - ãã®æŠ€è¡“ã®å®Ÿç”¨åŒ–æ™‚æœŸã‚’çœŸå‰£ã«è¿½ã£ã¦ã„ã‚‹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚„æŠ•è³‡å®¶
            
            ## åŸ·ç­†ãƒ­ã‚¸ãƒƒã‚¯ (Level 2 Granularity)
            - å˜ã«ã€Œå®Ÿç”¨åŒ–ã€ã ã‘ã§ãªãã€ã€ŒæŠ€è¡“çš„çµ¶å¯¾æ¡ä»¶ (Prerequisites)ã€ã®é”æˆåº¦ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã™ã‚‹ã€‚
            - ä¾‹: "å…¨å›ºä½“é›»æ± ã®å®Ÿç”¨åŒ–" ã§ã¯ãªã "é›»è§£è³ªä¼å°ç‡ 10mS/cm ã®é”æˆ" ã«æ³¨ç›®ã€‚
            
            ## æ§‹æˆæ¡ˆ
            1. **Impact Assessment (å½±éŸ¿è©•ä¾¡)**:
               - ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã‚’ã€ŒåŠ é€Ÿã€ã•ã›ãŸã‹ã€ã€Œé…å»¶ã€ã•ã›ãŸã‹ã€ãã‚Œã¨ã‚‚ã€Œç¶­æŒã€ã‹ã€‚
               - çµè«–ã‚’æœ€åˆã«æç¤ºã€‚
            2. **Technical Deep Dive (æŠ€è¡“çš„è©³ç´°)**:
               - å…·ä½“çš„ã«ä½•ã®ã‚¹ãƒšãƒƒã‚¯ãŒå‘ä¸Šã—ãŸã®ã‹ï¼ˆç²¾åº¦ã€é€Ÿåº¦ã€ã‚³ã‚¹ãƒˆã€è€ä¹…æ€§ï¼‰ã€‚
               - å¾“æ¥æŠ€è¡“ï¼ˆSOTAï¼‰ã¨ã®æ¯”è¼ƒã€‚
            3. **Bottleneck Analysis (ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ)**:
               - æ¬¡ã«æ®‹ã•ã‚Œã¦ã„ã‚‹èª²é¡Œã¯ä½•ã‹ï¼ˆé‡ç”£æ€§ã€æ³•è¦åˆ¶ã€é›»åŠ›æ¶ˆè²»ãªã©ï¼‰ã€‚
            4. **Revised Timeline (äºˆæ¸¬ä¿®æ­£)**:
               - ã“ã®æˆæœã«ã‚ˆã‚Šã€å®Ÿç”¨åŒ–äºˆæƒ³æ™‚æœŸãŒã©ã†å¤‰å‹•ã™ã‚‹ã‹ã€‚
               - "2028 Q3" ã®ã‚ˆã†ã«å…·ä½“çš„ã«è¨˜è¿°ã€‚
            
            ## åŸ·ç­†ãƒ«ãƒ¼ãƒ«
            - **ä¸€æ¬¡æƒ…å ±ä¸»ç¾©**: è«–æ–‡ã‚„å…¬å¼ãƒªãƒªãƒ¼ã‚¹ã«åŸºã¥ãäº‹å®Ÿã®ã¿ã‚’æ‰±ã†ã€‚å™‚ãƒ¬ãƒ™ãƒ«ã¯é™¤å¤–ã€‚
            - **å†·é™ãªè©•ä¾¡**: "é©å‘½çš„" "ç ´å£Šçš„" ã¨ã„ã£ãŸå½¢å®¹è©ã‚’é¿ã‘ã€æ•°å€¤ã§èªã‚‹ã€‚
            
            ## ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            - Markdownå½¢å¼
            - 3000æ–‡å­—ç¨‹åº¦
            - æŠ€è¡“ä»•æ§˜ã¯ãƒ†ãƒ¼ãƒ–ãƒ«ã§æ¯”è¼ƒ
            - HTMLã‚¿ã‚°ä½¿ç”¨ç¦æ­¢
            
            ## ã‚¿ã‚¤ãƒˆãƒ«ä½œæˆãƒ«ãƒ¼ãƒ«
            - **å½¢å¼**: [æŠ€è¡“å] [å‹•è©/çŠ¶æ…‹]ï¼š[å½±éŸ¿]
            - **ä¾‹**: å…¨å›ºä½“é›»æ± ã®é›»è§£è³ªåŠ£åŒ–å•é¡Œã‚’è§£æ±ºï¼š2027å¹´é‡ç”£ã¸ã®é“ç­‹
            """)
        }
        
        # Select prompt
        prompt = prompts.get(article_type, prompts["topic-focus"])
        
        if extra_instructions:
            prompt += f"\n\n{extra_instructions}\n"
        
        # Add common formatting instruction
        prompt += textwrap.dedent("""
        
        ## å‡ºåŠ›å½¢å¼
        å¿…ãšä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
        
        1è¡Œç›®: # [ç”Ÿæˆã—ãŸã‚¿ã‚¤ãƒˆãƒ«]
        2è¡Œç›®: ç©ºè¡Œ
        3è¡Œç›®ä»¥é™: è¨˜äº‹æœ¬æ–‡ï¼ˆå°å…¥ã‹ã‚‰å§‹ã‚ã‚‹ï¼‰
        
        **è¦‹å‡ºã—ãƒ¬ãƒ™ãƒ«:**
        - ã‚¿ã‚¤ãƒˆãƒ«: # (H1) â† è¨˜äº‹ã®ä¸»é¡Œ
        - å¤§è¦‹å‡ºã—: ## (H2) â† è¨˜äº‹ã®ä¸»è¦ãªæ§‹æˆè¦ç´ ï¼ˆç« ï¼‰
        - ä¸­è¦‹å‡ºã—: ### (H3) â† ç« ã‚’æ§‹æˆã™ã‚‹å…·ä½“çš„ãªãƒˆãƒ”ãƒƒã‚¯ï¼ˆç¯€ï¼‰
        
        **ã€é‡è¦ã€‘Markdownè¨˜è¿°ãƒ«ãƒ¼ãƒ«:**
        - **ãƒªã‚¹ãƒˆï¼ˆç®‡æ¡æ›¸ãï¼‰ã®å‰ã«ã¯å¿…ãšç©ºè¡Œã‚’å…¥ã‚Œã‚‹ã“ã¨ã€‚**
        - **ãƒã‚¹ãƒˆï¼ˆå…¥ã‚Œå­ï¼‰ã—ãŸãƒªã‚¹ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã¯å¿…ãšåŠè§’ã‚¹ãƒšãƒ¼ã‚¹4ã¤ï¼ˆ4 spacesï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚**
        
        ä¾‹:
        # ã€æŠ€è¡“è§£èª¬ã€‘æ¬¡ä¸–ä»£åŠå°ä½“ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°æŠ€è¡“ã®çªç ´å£
        
        ## 1. Executive Summary
        MITã®ç ”ç©¶ãƒãƒ¼ãƒ ãŒç™ºè¡¨ã—ãŸæ–°ã—ã„...
        
        ## 2. Technical Spec
        | é …ç›® | ä»Šå›ã®æˆæœ | å¾“æ¥æŠ€è¡“ |
        | :--- | :--- | :--- |
        | é…ç·šå¯†åº¦ | ... | ... |
        """)
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-3-pro-preview',
                contents=prompt
            )
            return response.text
        except Exception as e:
            print(f"Error generating content: {e}")
            return None

    def generate_image(self, prompt, output_path, aspect_ratio="16:9"):
        """
        Generate an image using Gemini 2.5 Flash Image (Primary) or Imagen 3.0 (Fallback).
        """
        # 1. Try Gemini 2.5 Flash Image (API Key supported)
        try:
            print(f"Generating image with Gemini 2.5 Flash Image for prompt: {prompt}")
            
            # Use google-genai SDK (v1beta) for API Key support and aspect ratio control
            client_v1beta = genai.Client(api_key=self.api_key, vertexai=False, http_options={'api_version': 'v1beta'})
            
            response = client_v1beta.models.generate_content(
                model='gemini-2.5-flash-image',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_modalities=["IMAGE"],
                    image_config=types.ImageConfig(
                        aspect_ratio=aspect_ratio,
                    )
                )
            )
            
            # Extract image from response (Gemini 2.5 Flash)
            if response.parts:
                for part in response.parts:
                    if part.inline_data is not None:
                        image_bytes = part.inline_data.data
                        with open(output_path, 'wb') as f:
                            f.write(image_bytes)
                        print(f"Image saved to: {output_path}")
                        return output_path
            
            print("No image found in Gemini 2.5 response, trying fallback...")
            
        except Exception as e:
            print(f"Gemini 2.5 Flash Image failed ({e}), falling back to Imagen 3.0...")

        # 2. Fallback to Imagen 3.0 (Vertex AI only)
        try:
            print(f"Generating image with Imagen 3.0 (Fallback) for prompt: {prompt}")
            
            response = self._retry_request(
                self.client.models.generate_images,
                model='imagen-3.0-generate-001',
                prompt=prompt,
                config={
                    'aspect_ratio': aspect_ratio
                }
            )
            
            # Extract image from response (Imagen 3.0)
            if response.generated_images:
                image_bytes = response.generated_images[0].image.image_bytes
                with open(output_path, 'wb') as f:
                    f.write(image_bytes)
                print(f"Image saved to: {output_path}")
                return output_path
            
            print("No image generated in Imagen 3.0 response.")
            return None
                
        except Exception as e:
            print(f"Failed to generate image with Imagen 3.0: {e}")
            return None


    def generate_image_prompt(self, title, content_summary, article_type="topic-focus"):
        """
        Generate an optimized English image prompt based on article title and content.
        
        Args:
            title: Article title
            content_summary: Brief summary
            article_type: Type of article (topic-focus, daily-briefing)
        
        Returns:
            English image prompt optimized for Imagen 3.0/Gemini 2.5
        """
        prompt = textwrap.dedent(f"""
        You are an expert at creating image generation prompts for Imagen 3.0.
        
        Based on the following article information, create a detailed English image prompt that:
    1. **Theme**: "Future Technology", "Innovation", "Cyberpunk/High-Tech Sci-Fi".
    2. **Visual Metaphors**: 
       - **Futuristic**: Neon lights, data streams, holograms, advanced robotics, space exploration.
       - **Abstract**: Geometric patterns, circuit boards, glowing nodes connecting (representing networks).
    3. **Style**: Premium, Editorial, "Wired Magazine" or "The Verge" feature image style. High contrast, vibrant colors (Cyan, Magenta, Deep Blue).
    4. **Lighting**: Cinematic, Volumetric lighting, Ray tracing vibes.
    5. Avoids text, human faces, or complex diagrams.
    
    Article Title: {title}
    Article Type: {article_type}
    Content Summary: {content_summary[:2000]}
    
    Generate a single, detailed English image prompt (max 100 words) that would create a compelling hero image for this article.
    Output ONLY the prompt text, no explanations.
        """)
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-3-pro-preview',
                contents=prompt
            )
            return response.text.strip()
        except Exception as e:
            print(f"Error generating image prompt: {e}")
            # Fallback to simple prompt
            return f"Futuristic technology background related to {title}, cyberpunk style, high quality, 4k"



    def generate_static_page(self, page_type):
        """
        Generate static page content (privacy policy, about, contact).
        
        Args:
            page_type: "privacy", "about", or "contact"
        
        Returns:
            Generated markdown content
        """
        prompts = {
            "privacy": textwrap.dedent("""
            ã‚ãªãŸã¯æ³•å‹™ã«è©³ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
            ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€æ—¥æœ¬ã®å€‹äººæƒ…å ±ä¿è­·æ³•ã«æº–æ‹ ã—ãŸãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            
            ã€ã‚µã‚¤ãƒˆæƒ…å ±ã€‘
            - ã‚µã‚¤ãƒˆå: TechShiftï¼ˆãƒ†ãƒƒã‚¯ã‚·ãƒ•ãƒˆï¼‰
            - é‹å–¶è€…: TechShiftç·¨é›†éƒ¨
            - è¨­ç«‹: 2026å¹´1æœˆ
            - ç›®çš„: æŠ€è¡“ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã®å¯è¦–åŒ–ãƒ»äºˆæ¸¬æƒ…å ±ã®æä¾›
            - ä½¿ç”¨æŠ€è¡“: Googleã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ã€Cookie
            - ãŠå•ã„åˆã‚ã›: info@finshift.net
            
            ## å«ã‚ã‚‹ã¹ãé …ç›®
            1. å€‹äººæƒ…å ±ã®å–ã‚Šæ‰±ã„ã«ã¤ã„ã¦
            2. åé›†ã™ã‚‹æƒ…å ±ã®ç¨®é¡ï¼ˆã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ã€Cookieç­‰ï¼‰
            3. åˆ©ç”¨ç›®çš„ï¼ˆã‚µã‚¤ãƒˆæ”¹å–„ã€çµ±è¨ˆåˆ†æç­‰ï¼‰
            4. ç¬¬ä¸‰è€…æä¾›ï¼ˆGoogleã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹ç­‰ï¼‰
            5. Cookieãƒ»ã‚¢ã‚¯ã‚»ã‚¹è§£æãƒ„ãƒ¼ãƒ«ã«ã¤ã„ã¦
            6. å…è²¬äº‹é …ï¼ˆæœ¬ã‚µã‚¤ãƒˆã®æƒ…å ±ã¯äºˆæ¸¬ã§ã‚ã‚Šã€æ­£ç¢ºæ€§ã‚’ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ãªã„æ—¨ï¼‰
            7. å€‹äººæƒ…å ±ã®é–‹ç¤ºãƒ»è¨‚æ­£ãƒ»å‰Šé™¤ã«ã¤ã„ã¦
            8. ãŠå•ã„åˆã‚ã›å…ˆ
            9. åˆ¶å®šæ—¥ãƒ»æ”¹å®šæ—¥
            
            ## å‡ºåŠ›å½¢å¼
            - Markdownå½¢å¼ã§å‡ºåŠ›
            - è¦‹å‡ºã—ã¯H2ï¼ˆ##ï¼‰ã¨H3ï¼ˆ###ï¼‰ã‚’ä½¿ç”¨
            - ç®‡æ¡æ›¸ãã‚„è¡¨ã‚’é©å®œä½¿ç”¨
            - æ³•çš„ã«æ­£ç¢ºã§ã€ã‹ã¤èª­ã¿ã‚„ã™ã„æ–‡ç« 
            - æœ€å¾Œã«ã€Œåˆ¶å®šæ—¥: 2026å¹´1æœˆ1æ—¥ã€ã‚’è¨˜è¼‰
            
            ## æ³¨æ„ç‚¹
            - å°‚é–€ç”¨èªã¯åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
            - **æŠ€è¡“äºˆæ¸¬ãƒ»æŠ•è³‡åˆ¤æ–­ã¯è‡ªå·±è²¬ä»»**ã§ã‚ã‚‹ã¨ã„ã†å…è²¬ã‚’å¿…ãšå«ã‚ã‚‹
            - é€£çµ¡å…ˆã‚’æ˜è¨˜
            """),
            
            "about": textwrap.dedent("""
            ã‚ãªãŸã¯ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€TechShiftã®é‹å–¶è€…æƒ…å ±ãƒšãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            
            ã€ã‚µã‚¤ãƒˆæƒ…å ±ã€‘
            - ã‚µã‚¤ãƒˆå: TechShiftï¼ˆãƒ†ãƒƒã‚¯ã‚·ãƒ•ãƒˆï¼‰
            - é‹å–¶è€…: TechShiftç·¨é›†éƒ¨
            - è¨­ç«‹: 2026å¹´1æœˆ
            - ãŠå•ã„åˆã‚ã›: info@finshift.net
            
            ã€ãƒ“ã‚¸ãƒ§ãƒ³: "Dynamic Navigation Chart"ã€‘
            æ—¢å­˜ã®é™çš„ãªãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ï¼ˆPDFï¼‰ã‚’éå»ã®ã‚‚ã®ã¨ã—ã€
            æ—¥ã€…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è§£æã‚’é€šã˜ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›´æ–°ã•ã‚Œç¶šã‘ã‚‹ã€Œå‹•çš„èˆªæµ·å›³ã€ã‚’æä¾›ã™ã‚‹ã€‚
            
            ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèª­è€…: "Visionary Practitioner"ã€‘
            ä¼æ¥­ã®CTOã€R&Dè²¬ä»»è€…ã€æ–°è¦äº‹æ¥­æ‹…å½“è€…ã€‚
            æŠ€è¡“ã®é€²åŒ–ãŒé€Ÿã™ãã¦æˆ¦ç•¥ãŒé™³è…åŒ–ã™ã‚‹ä¸å®‰ã‚’æŒã¤ãƒªãƒ¼ãƒ€ãƒ¼ãŸã¡ã€‚
            
            ã€ä¸»ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
            - Daily Briefing: å…¨ã‚»ã‚¯ã‚¿ãƒ¼ã‚’æ¨ªæ–­ã—ãŸæ—¥åˆŠæŠ€è¡“äºˆæ¸¬
            - Deep Dive: ç‰¹å®šæŠ€è¡“ï¼ˆå…¨å›ºä½“é›»æ± ã€AGIç­‰ï¼‰ã®å®Ÿç”¨åŒ–æ™‚æœŸäºˆæ¸¬
            - Roadmap: å‹•çš„ã«å¤‰åŒ–ã™ã‚‹æŠ€è¡“ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã®å¯è¦–åŒ–
            
            ## å«ã‚ã‚‹ã¹ãé …ç›®
            1. TechShiftã«ã¤ã„ã¦ï¼ˆã‚µã‚¤ãƒˆã®ç›®çš„ãƒ»ãƒ“ã‚¸ãƒ§ãƒ³ã€ŒDynamic Navigation Chartã€ï¼‰
            2. åŸºæœ¬æƒ…å ±ï¼ˆã‚µã‚¤ãƒˆåã€é‹å–¶è€…ã€è¨­ç«‹å¹´ã€ãŠå•ã„åˆã‚ã›å…ˆï¼‰ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§
            3. ãƒŸãƒƒã‚·ãƒ§ãƒ³ãƒ»æä¾›ä¾¡å€¤ï¼ˆç‚¹ã§ã¯ãªãç·šã§è¦‹ã‚‹ã€ä¸€æ¬¡æƒ…å ±ã®å¾¹åº•ã€æ³¢åŠã®å¯è¦–åŒ–ï¼‰
            4. ä¸»ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚«ãƒ†ã‚´ãƒªã®ç´¹ä»‹
            5. æƒ³å®šèª­è€…
            6. ãŠå•ã„åˆã‚ã›å…ˆ
            
            ## å‡ºåŠ›å½¢å¼
            - Markdownå½¢å¼ã§å‡ºåŠ›
            - è¦‹å‡ºã—ã¯H2ï¼ˆ##ï¼‰ã¨H3ï¼ˆ###ï¼‰ã‚’ä½¿ç”¨
            - åŸºæœ¬æƒ…å ±ã¯Markdownãƒ†ãƒ¼ãƒ–ãƒ«ã§æ•´ç†
            - å…ˆé€²çš„ã§ã€ä¿¡é ¼æ„ŸãŒã‚ã‚Šã€ãƒ“ã‚¸ãƒ§ãƒŠãƒªãƒ¼ãªæ–‡ç« 
            - æœªæ¥ã¸ã®æƒ…ç†±ãŒä¼ã‚ã‚‹å†…å®¹
            """),
            
            "contact": textwrap.dedent("""
            ã‚ãªãŸã¯ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µãƒãƒ¼ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚
            ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€TechShiftã®ãŠå•ã„åˆã‚ã›ãƒšãƒ¼ã‚¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            
            ã€ã‚µã‚¤ãƒˆæƒ…å ±ã€‘
            - ã‚µã‚¤ãƒˆå: TechShiftï¼ˆãƒ†ãƒƒã‚¯ã‚·ãƒ•ãƒˆï¼‰
            - é‹å–¶è€…: TechShiftç·¨é›†éƒ¨
            - ãŠå•ã„åˆã‚ã›: info@finshift.net
            - å¯¾å¿œæ™‚é–“: å¹³æ—¥ 10:00-18:00ï¼ˆåœŸæ—¥ç¥æ—¥ã‚’é™¤ãï¼‰
            
            ## å«ã‚ã‚‹ã¹ãé …ç›®
            1. ãŠå•ã„åˆã‚ã›ã«ã¤ã„ã¦ï¼ˆå°å…¥æ–‡ï¼‰
            2. ãŠå•ã„åˆã‚ã›æ–¹æ³•ï¼ˆãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ï¼‰
            3. å¯¾å¿œæ™‚é–“
            4. ãŠå•ã„åˆã‚ã›å†…å®¹ã®ä¾‹ï¼ˆè¨˜äº‹ã®å†…å®¹ç¢ºèªã€ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹é€ä»˜ã€å–æä¾é ¼ãªã©ï¼‰
            5. è¿”ä¿¡ã¾ã§ã®ç›®å®‰æ™‚é–“
            6. æ³¨æ„äº‹é …ï¼ˆå€‹äººæƒ…å ±ã®å–ã‚Šæ‰±ã„ãªã©ï¼‰
            
            ## å‡ºåŠ›å½¢å¼
            - Markdownå½¢å¼ã§å‡ºåŠ›
            - è¦‹å‡ºã—ã¯H2ï¼ˆ##ï¼‰ã¨H3ï¼ˆ###ï¼‰ã‚’ä½¿ç”¨
            - ç®‡æ¡æ›¸ãã‚’é©å®œä½¿ç”¨
            - ä¸å¯§ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« 
            - ãŠå•ã„åˆã‚ã›ã—ã‚„ã™ã„é›°å›²æ°—
            
            ## æ³¨æ„ç‚¹
            - ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯å¿…ãšè¨˜è¼‰
            - å¯¾å¿œæ™‚é–“ã‚’æ˜è¨˜
            - ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¡ˆå†…
            """)
        }
        
        prompt = prompts.get(page_type)
        if not prompt:
            raise ValueError(f"Invalid page_type: {page_type}. Must be 'privacy', 'about', or 'contact'")
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-3-pro-preview',
                contents=prompt
            )
            return response.text
        except Exception as e:
            print(f"Error generating static page: {e}")
            return None


    def generate_structured_summary(self, content):
        """
        Generate a structured JSON summary of the article for internal linking relevance.
        
        [USAGE ROLE]: Output Processing
        This method is used *AFTER* generation to extract metadata from the **INTERNAL** article.
        The result is saved in WordPress 'ai_structured_summary' field.
        """
        prompt = textwrap.dedent(f"""
        You are an expert content analyst. Analyze the following tech article and generate a structured summary in JSON format.
        This summary will be used by an AI system to identify relevant internal links.
        IMPORTANT: The content is Japanese, so the 'summary' and 'key_topics' MUST be written in Japanese.

        Article Content:
        {content[:4000]}... (truncated)

        Output JSON format (Strictly JSON only):
        {{
            "summary": "Detailed summary of the article content (300-500 chars) in Japanese. Focus on technical specifics and timeline impacts.",
            "key_topics": ["list", "of", "specific", "technologies", "covered", "(in Japanese)"],
            "entities": ["list", "of", "companies", "products", "or", "research_labs", "mentioned", "(preserve original names)"],
            "timeline_impact": "Brief description of how this affects the roadmap (Accelerated/Delay/Unchanged) in Japanese.",
            "technical_bottleneck": "Describe the technical bottleneck mentioned (if any) in Japanese."
        }}
        """)
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-3-pro-preview',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            import json
            return json.loads(response.text)
        except Exception as e:
            print(f"Structured summary generation failed: {e}")
            return None

    def generate_sns_content(self, title, content, article_type="market-analysis"):
        """
        Generate engaging SNS (Twitter/X) post content.
        Output is JSON: {"hook": "...", "summary": "...", "hashtags": ["#tag1", ...]}
        """
        # Truncate content for efficiency
        truncated_content = content[:3000]
        
        prompt = textwrap.dedent(f"""
        You are an expert social media manager for a futuristic tech media site "TechShift".
        Create an engaging X (Twitter) post content based on the following article.
        
        Target Audience: CTOs, R&D Leaders, Tech Investors (Visionary Practitioners).
        Goal: Maximize Engagement by appealing to "Deep Insight" and "Future Impact".
        
        Article Title: {title}
        Article Type: {article_type}
        Content (excerpt):
        {truncated_content}
        
        Requirements:
        1. **Hook**: A provocative opening. Focus on "What changed in the future timeline".
           - MUST include 1 relevant emoji (ï¿½, ğŸ§¬, ğŸ¤–, âš›ï¸, etc.).
           - Max 60 chars.
        2. **Summary**: Intellectual teaser. "Why this matters for the roadmap".
           - Focus on solving bottlenecks or new possibilities.
           - Max 120 chars.
        3. **Hashtags**: 5 hashtags optimized for Tech Communities.
           - **Required**:
             - Relevant Tech Keywords (e.g., #GenAI, #Quantum, #SolidStateBattery).
             - **Middle Words**: #æŠ€è¡“æˆ¦ç•¥, #æœªæ¥äºˆæ¸¬, #R_and_D (Avoid just #Technology).
             - Specific companies/labs if mentioned.
           - **Goal**: Reach professionals, not casual readers.
        
        4. Language: Japanese. 
        5. **Tone**: Visionary, Concise, Leading-edge. "Don't just read the news, read the future."
        
        Output JSON format (Strictly JSON only):
        {{
            "hook": "ğŸš€ GPT-5ã®æ¨è«–èƒ½åŠ›ãŒã€å‰µè–¬ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ã€Œ2å¹´ã€çŸ­ç¸®ã™ã‚‹ã€‚",
            "summary": "å¾“æ¥ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã¯éå»ã®ã‚‚ã®ã«ã€‚ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ãŒè§£æ±ºã—ãŸã€Œ3ã¤ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€ã¨ã¯ï¼Ÿ",
            "hashtags": ["#GenerativeAI", "#DrugDiscovery", "#NVIDIA", "#BioTech", "#æŠ€è¡“æˆ¦ç•¥"]
        }}
        """)
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-3-pro-preview',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            import json
            return json.loads(response.text)
        except Exception as e:
            print(f"SNS content generation failed: {e}")
            # Fallback
            return {
                "hook": f"{title}",
                "summary": "æœ€æ–°ã®æŠ€è¡“å‹•å‘ã¨ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã¸ã®å½±éŸ¿ã‚’åˆ†æã—ã¾ã—ãŸã€‚",
                "hashtags": ["#TechShift", "#DeepTech", "#FutureTrends", "#Innovation"]
            }

    def check_duplication(self, new_title, new_summary, existing_titles):
        """
        Check if a new article title semantically matches any existing titles.
        
        Args:
            new_title: The title of the potential new article
            new_summary: The summary of the potential new article
            existing_titles: List of existing article titles (WP posts + currently generated)
            
        Returns:
            The matching existing title if duplicate found, or None.
        """
        if not existing_titles:
            return None
            
        # Optimization: Don't check against massive lists if unnecessary.
        # But for now, we assume existing_titles is reasonably sized (e.g., < 50).
        
        prompt = f"""
        You are a duplicate content detector for a technology foresight media "TechShift".
        Determine if the "New Article" covers the **same specific news topic** as any of the "Existing Articles".
        
        Rule:
        - Return the EXACT title of the existing article ONLY if they are about the same specific news event or announcement.
        - If the new article is just a general topic match (e.g. both are about "Quantum Computing") but different specific news, return "None".
        - If the new article is a "Summary" or "Compilation" and the existing one is a single news, they are different -> return "None".
        - Different companies doing similar things are DIFFERENT -> return "None".
        - Same company doing the same thing (reported by different sources) are DUPLICATES -> return the existing title.
        
        New Article:
        Title: "{new_title}"
        Summary: "{new_summary}"
        
        Existing Articles:
        {json.dumps(existing_titles, ensure_ascii=False, indent=2)}
        
        Output JSON format:
        {{
            "is_duplicate": true/false,
            "duplicate_of": "Exact Title of Existing Article" (or null if false),
            "reason": "Brief explanation"
        }}
        """
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-2.0-flash-exp',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            result = json.loads(response.text)
            
            if result.get("is_duplicate"):
                print(f"Duplicate detected! '{new_title}' is duplicate of '{result.get('duplicate_of')}'")
                print(f"Reason: {result.get('reason')}")
                return result.get("duplicate_of")
            
            return None
            
        except Exception as e:
            print(f"Duplication check failed: {e}")
            return None

    # --- Daily Briefing Methods ---



    def check_relevance_batch(self, articles):
        """
        Check relevance for a batch of articles (list of dicts).
        Each dict must have 'url_hash', 'title', 'summary'.
        
        Returns: Dict mapping url_hash -> {'is_relevant': bool, 'reason': str}
        """
        if not articles:
            return {}
            
        # Prepare input list for prompt
        input_list = []
        for art in articles:
            input_list.append({
                "id": art.get('url_hash', 'unknown'),
                "title": art.get('title', ''),
                "summary": art.get('summary', '')[:500] 
            })
            
        prompt = f"""
        You are a "Deep Tech" news filter for TechShift.
        Analyze the following list of articles and determine if EACH one is relevant to **Engineering, R&D, Science, or Future Technology Analysis**.
        
        Criteria for "Relevant":
        - **Scientific Breakthroughs**: New discoveries in AI, Quantum, Bio, Energy, Space.
        - **Engineering Milestones**: Successful tests (e.g., Starship launch), pilot plant openings, efficiency records.
        - **Strategic Business**: M&A in deep tech, huge VC funding rounds, major government regulation on tech.
        
        Criteria for "Not Relevant" (Noise):
        - **Consumer Gadgets**: Smartwatch reviews, minor iPhone updates (unless revolutionary).
        - **General Politics**: Elections (unless directly affecting tech policy).
        - **Celebrity/Gossip**: Tech CEO scandals (unless affecting company roadmap).
        - **Short-term Stock Noise**: Daily price fluctuation without fundamental news.
        
        Input Articles:
        {json.dumps(input_list, ensure_ascii=False, indent=2)}
        
        Output JSON:
        A list of objects, one for each input article:
        [
            {{
                "id": "article_id_from_input",
                "is_relevant": true/false,
                "reason": "Brief explanation"
            }},
            ...
        ]
        """
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-2.0-flash-exp', 
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            res_json = json.loads(response.text)
            
            # Map back to url_hash
            result_map = {}
            for res in res_json:
                u_hash = res.get('id')
                if u_hash:
                    result_map[u_hash] = {
                        'is_relevant': res.get('is_relevant', False),
                        'reason': res.get('reason', 'Unknown')
                    }
            return result_map
            
        except Exception as e:
            print(f"Batch relevance check failed: {e}")
            # Fallback: Mark all as Relevant with error note
            fallback_map = {}
            for art in articles:
                fallback_map[art.get('url_hash')] = {
                    'is_relevant': True,
                    'reason': f"Batch AI Check Failed: {e}"
                }
            return fallback_map

    def analyze_tech_impact(self, context_news_list, market_data_str, economic_events_str, region, extra_context=""):
        """
        Analyze tech news AND market data to generate "Future Insight" (Evolution Phase & Timeline Impact).
        Returns: JSON with evolution_phase, timeline_impact, hero_topic, scenarios.
        """
        news_text = "\n".join([f"- [{art['published_at']}] {art['title']}: {art['summary'][:200]}" for art in context_news_list])
        
        prompt = textwrap.dedent(f"""
        You are the "Future Foresight Engine" for TechShift. Analyze the provided data for the **{region}** region.
        
        ## Input Data
        
        ### 1. Macro Context (Investment Climate)
        - Market/Econ Data: {market_data_str}
        - Note: Use this to judge "VC Funding Environment" or "Cost of Capital" for Deep Tech.
        
        ### 2. Tech News (Last 24h)
        {news_text}
        
        ### 3. Context & Continuity
        {extra_context}
        
        ## Analysis Tasks
        1. **Evolution Phase**: Define the current global tech phase in **Japanese**.
           - Examples: "ç”ŸæˆAIã®å¹»æ»…æœŸå…¥ã‚Š", "é‡å­æŠ€è¡“ã®å®Ÿç”¨åŒ–å‰å¤œ", "å®‡å®™é–‹ç™ºã®å•†æ¥­åŒ–åŠ é€Ÿ".
        2. **Timeline Impact**: 0 (Severe Delay) to 100 (Massive Acceleration). 50 is Neutral.
        3. **Hero Topic**: What single technology/topic is most critical today?
           - **Consistency Check**: Reference "Context". Is this a follow-up to yesterday's breakthrough?
        
        4. **Scenarios (The TechShift Logic)**:
           - **Main Scenario (Base Case)**: Most likely timeline evolution.
             - **Format**: "Event -> Impact on Roadmap".
             - **Example**: "OpenAIã®æ–°ãƒ¢ãƒ‡ãƒ«ç™ºè¡¨ -> ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–‹ç™ºãŒåŠ é€Ÿã—ã€2025å¹´ã®å®Ÿç”¨åŒ–äºˆæ¸¬ã‚’ç¶­æŒ".
           - **Accelerated Case (Bull)**: What could speed things up?
             - **Example**: "è¦åˆ¶ç·©å’ŒãŒæ—©æœŸã«å®Ÿç¾ -> ãƒ‰ãƒ­ãƒ¼ãƒ³é…é€ã®å…¨å›½å±•é–‹ãŒ1å¹´å‰å€’ã—".
           - **Delayed Case (Bear)**: What are the risks/bottlenecks?
             - **Example**: "GPUä¾›çµ¦ä¸è¶³ãŒé•·æœŸåŒ– -> å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒåœæ»ã—ã€AGIåˆ°é”ãŒé…å»¶".
           
           - **Probability**: Assign stats (Must sum to 100%).
        
        5. **AI Structured Summary**:
           - **summary**: Concise summary in **Japanese**.
           - **key_topics**: List of 3-5 tech keywords in **Japanese**.

        ## Output JSON
        {{
            "evolution_phase": "AIå®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º (Implementation Phase)",
            "timeline_impact": 75, # 0-100 Impact Score (High = Acceleration)
            "impact_label": "Accelerated", # Label (Accelerated/Delayed/Neutral)
            "hero_topic": "...", # Hero Topic (formerly primary_driver)
            "scenarios": {{
                "review": "Verification of yesterday's view. e.g., 'The bottleneck identified yesterday is resolving...'",
                "main": {{ 
                    "condition": "Main timeline projection", 
                    "probability": "60%"
                }},
                "bull": {{ 
                    "condition": "Acceleration scenario", 
                    "probability": "20%"
                }},
                "bear": {{ 
                    "condition": "Delay/Bottleneck scenario", 
                    "probability": "20%"
                }},
                "mid_term": {{ 
                    "view": "Positive/Neutral/Negative", 
                    "events": "CES 2026, SpaceX Launch", 
                    "risk": "Regulatory Change" 
                }}
            }},
            "ai_structured_summary": {{
                "summary": "...",
                "key_topics": ["...", "..."]
            }},
            "reasoning": "Reasoning for the impact score"
        }}
        """)
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-3-pro-preview', # High reasoning model
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json"
                )
            )
            result = json.loads(response.text)
            if isinstance(result, list):
                if len(result) > 0:
                    return result[0]
                else:
                    return {}
            return result
        except Exception as e:
            print(f"Tech Impact Analysis failed: {e}")
            return None

    def write_briefing(self, analysis_result, region, context_news=None, market_data_str=None, events_str=None, date_str=None, internal_links_context=None):
        """
        [Daily Briefing Pipeline]
        Write the final Daily Briefing article in Markdown based on the analysis and raw context.
        """
        # Prepare context strings
        news_text = ""
        if context_news:
             news_text = "\n".join([f"- {art['title']}" for art in context_news[:10]])

        prompt = textwrap.dedent(f"""
        You are the Editor-in-Chief of "TechShift". Write a "Daily Briefing" for the **{region}** region.
        
        ## Input Data
        
        ### 1. Future Insight (AI Strategy)
        {json.dumps(analysis_result, ensure_ascii=False, indent=2)}
        
        ### 2. Macro Context (Raw Data)
        {market_data_str if market_data_str else "N/A"}
        
        ### 3. Key Tech News
        {news_text if news_text else "N/A"}
        
        ### 4. Internal Links (Reference)
        {internal_links_context if internal_links_context else "N/A"}
        
        ## Goal
        Create a **Visionary, Insightful, and Logic-Driven** report.
        Focus on "Connecting Dots" (Line) rather than just reporting news (Point).
        
        ## Tone & Style
        - **Visionary & Sharp**: "Wired" meets "McKinsey Report".
        - **Japanese Language**: High-quality, professional Japanese.
        - **Data-Backed**: Cite specs, funding amounts, and dates.
        
        ## Output Structure (Japanese Headers)
        
        1. **Title**: Generated based on rules below.
        
        2. **ã€The Shiftã€‘ (ä»Šæ—¥ã®çµè«–)**
           - Define how the "Humanity's Timeline" shifted today.
           - e.g., "AGI is 6 months closer," "Fusion power hits a cost wall."
           - **Bold** the key conclusion.
        
        3. **ã€Hero Topic: {analysis_result.get('hero_topic', analysis_result.get('primary_driver', 'Top Story'))}ã€‘**
           - Deep dive into the most important topic.
           - **Logic Chain**: Fact -> Analysis -> Future Impact.
           - Use `### æ˜¨æ—¥ã®ã‚·ãƒŠãƒªã‚ªæ¤œè¨¼` (Verification) here to check continuity if relevant.
        
        4. **ã€Cross-Sector Signalsã€‘ (ä»–åˆ†é‡ã¸ã®æ³¢åŠ)**
           - How does today's hero topic affect other sectors?
           - e.g., "AI improvement -> Robotics training acceleration."
           - Use bullet points.
        
        5. **ã€Reality Checkã€‘ (å®Ÿæ…‹æ¤œè¨¼)**
           - **Bottleneck Analysis**: What stops this from happening tomorrow? (Cost, Law, Energy).
           - **Cost of Capital**: Mention Macro Environment (Interest rates) influence on VC/Deep Tech if relevant.
        
        6. **ã€Scenariosã€‘ (æœªæ¥äºˆæ¸¬)**
           - **Scenarios Table**: Create a Markdown table comparing Main, Accelerated, and Delayed scenarios.
           - **Mid-term Outlook**: Next big events (Conferences, Keep Launches).
        
        ## Internal Linking
        - Integrate relevant links from "Internal Links Context" naturally into the text.
        - Format: `[Title](URL)`
        
        ## Title Rules
        1. **Format**: ã€Daily Shiftã€‘[Hero Topic]ãŒå¤‰ãˆãŸæœªæ¥ï½œ[Sub-Title]
        2. **Keywords**: Concrete names (OpenAI, Toyota, Starship).
        3. **Example**: ã€Œã€Daily Shiftã€‘GPT-5ãŒå‰µè–¬ã‚’2å¹´åŠ é€Ÿï½œã€Œå®Ÿé¨“å®¤ã€ã‹ã‚‰ã€Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼ã€ã¸ã€
        
        ## Format Rules
        - Markdown, approx 4000 chars.
        - **First line MUST be Title** (with #).
        - No HTML.
        - Use Japanese headers (e.g., ## ã€The Shiftã€‘).
        """)
        
        try:
            response = self._retry_request(
                self.client.models.generate_content,
                model='gemini-3-pro-preview',
                contents=prompt
            )
            return response.text
        except Exception as e:
            print(f"Briefing writing failed: {e}")
            return None

if __name__ == "__main__":
    # Test generation
    try:
        client = GeminiClient()
        print("GeminiClient initialized successfully.")
    except Exception as e:
        print(f"Initialization failed: {e}")
